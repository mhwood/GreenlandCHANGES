{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download MEaSUREs Annual Antarctic Ice Velocity Maps, Version 1 (NSIDC-0720)\n",
    "\n",
    "\n",
    "NSICD MEaSUREs Annual Antarctic Ice Velocity Maps, Version 1 (NSIDC-0720):\n",
    "\n",
    "https://nsidc.org/data/nsidc-0720/versions/1  <br><br>\n",
    "\n",
    "\n",
    "\n",
    "NASA EarthData Data Access (echo_collection_id=C2245171699-NSIDC_ECS):\n",
    "\n",
    "https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2245171699-NSIDC_ECS&pg[0][v]=f&pg[0][gsk]=-start_date&q=NSIDC-0720%20V001 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import requests\n",
    "import os\n",
    "#import netCDF4 as nc4\n",
    "#from osgeo import gdal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set options\n",
    "\n",
    "First, set collections ID for desired data set from NASA EarthData and set the region name.\n",
    "\n",
    "Example: Getting a collection ID from EarthData Search: \n",
    "[MEaSUREs Annual Antarctic Ice Velocity Maps V001](https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2245171699-NSIDC_ECS&pg[0][v]=f&pg[0][gsk]=-start_date&q=NSIDC-0720%20V001&tl=1686700071.247!3!!)\n",
    "\n",
    "While viewing the collection on EarthData, as in the above link, follow \"View More Info\" to visit the CMR page for the collection.\n",
    "Then, look for the collections ID in the URL or as a tag below the title. \n",
    "\n",
    "![Locating Collection ID from CMR page](getting_collectionID.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = 'C2245171699-NSIDC_ECS' # MEaSUREs Annual Antarctic\n",
    "region_name = 'Antarctic'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define two directories on your local drive as follows:\n",
    "\n",
    "| directory | purpose | \n",
    "|-----------|---------|\n",
    "|`project_folder` | This is the path where output data from the changes module will be stored - the data to be used directly for analysis. | \n",
    "|`data_folder` | This is the path where ice velocity and elevation data, from their respective sources, will be stored. The data_folder option was created to facilitate data storage on external drives. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/Users/tara/Documents/SJSU/MLML/Projects/CHANGES/Examples'\n",
    "data_folder='/Volumes/Seagate/CHANGES/data_repository/tutorial'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the IcesheetCHANGES object - this object will contain all pertinent information to initialize the data grids in your region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IcesheetCHANGES:\n",
    "    def __init__(self, project_folder, data_folder, region_name):\n",
    "\n",
    "        #this initiates the global domain and all of its associated parameters\n",
    "        self.project_folder = project_folder\n",
    "        self.data_folder = data_folder\n",
    "        self.region_name = region_name\n",
    "        self.velocity_grid_x = []\n",
    "        self.velocity_grid_y = []\n",
    "        self.elevation_grid_y = []\n",
    "        self.elevation_grid_x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "IC = IcesheetCHANGES(project_folder,data_folder, region_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain list of available files for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is from the ICESat2 data page\n",
    "def cmr_filter_urls(search_results):\n",
    "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
    "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
    "        return []\n",
    "\n",
    "    \n",
    "    entries = [e['links']\n",
    "                for e in search_results['feed']['entry']\n",
    "                if 'links' in e]\n",
    "    # Flatten \"entries\" to a simple list of links\n",
    "    links = list(itertools.chain(*entries))\n",
    "\n",
    "    urls = []\n",
    "    unique_filenames = set()\n",
    "    for link in links:\n",
    "        if 'href' not in link:\n",
    "            # Exclude links with nothing to download\n",
    "            continue    # continue jumps to next iteration in the loop\n",
    "        if 'inherited' in link and link['inherited'] is True:\n",
    "            # Why are we excluding these links?\n",
    "            continue\n",
    "        if 'rel' in link and 'data#' not in link['rel']:\n",
    "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
    "            continue\n",
    "        if 'title' in link and 'opendap' in link['title'].lower():\n",
    "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
    "            # This is a hack; when the metadata is updated to properly identify\n",
    "            # non-datapool links, we should be able to do this in a non-hack way\n",
    "            continue\n",
    "\n",
    "        filename = link['href'].split('/')[-1]\n",
    "        if filename in unique_filenames:\n",
    "            # Exclude links with duplicate filenames (they would overwrite)\n",
    "            continue\n",
    "        unique_filenames.add(filename)\n",
    "\n",
    "        urls.append(link['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully obtained 32 URLs.\n"
     ]
    }
   ],
   "source": [
    "# Build and call the CMR API URL\n",
    "cmr_query_url = 'https://cmr.earthdata.nasa.gov/search/granules.json?echo_collection_id=' + collection_id + '&page_size=2000'\n",
    "response = requests.get(cmr_query_url)\n",
    "\n",
    "# print error code based on response\n",
    "if response.status_code != 200:\n",
    "    print('ERROR: {}'.format(response.status_code))\n",
    "search_page = response.json()\n",
    "# If JSON contains an error message, print the message at the key, 'error'\n",
    "if 'errors' in search_page:\n",
    "    print(search_page['errors'])\n",
    "else: \n",
    "    urls = cmr_filter_urls(search_page)\n",
    "    print(\"Successfully obtained {} URLs.\".format(len(urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only .nc files and also store the file names in a seperate list\n",
    "measures_antarctic_IVM_file_links = []\n",
    "measures_antarctic_IVM_file_names = []\n",
    "metadata = []\n",
    "for url in urls:\n",
    "    if '.nc'in url and not 'xml' in url:\n",
    "        measures_antarctic_IVM_file_links.append(url)\n",
    "        url_parts = url.split('/')\n",
    "        file_name = url_parts[-1]\n",
    "        measures_antarctic_IVM_file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write available file names and links to csv file\n",
    "if not os.path.exists(os.path.join(IC.project_folder,IC.region_name,'Velocity','Metadata')):\n",
    "    os.makedirs(os.path.join(IC.project_folder,IC.region_name,'Velocity','Metadata'))\n",
    "f = open(os.path.join(IC.project_folder,IC.region_name,'Velocity','Metadata',\n",
    "                                IC.region_name + '_MEaSUREs_Antarctic_annual.csv'),'w')\n",
    "\n",
    "f.write('File_Name,URL')\n",
    "for ea in range(len(measures_antarctic_IVM_file_names)):\n",
    "    f.write('\\n'+measures_antarctic_IVM_file_names[ea]+','+measures_antarctic_IVM_file_links[ea])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing files, get a list of files not on disk\n",
    "def obtain_download_list(file_names, file_links):\n",
    "    \n",
    "    output_folder=os.path.join(IC.data_folder,'Velocity','MEaSUREs','Annual','Data')\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    download_list_names=[]\n",
    "    download_list_links=[]\n",
    "    for fil in file_names:\n",
    "        download_file = True\n",
    "        if fil not in os.listdir(os.path.join(output_folder)):\n",
    "            for existing_fil in os.listdir(os.path.join(output_folder)):\n",
    "                if fil == existing_fil:  # this allows for older versions to be kept \n",
    "                    download_file = False\n",
    "        else:\n",
    "            download_file=False\n",
    "        if download_file:\n",
    "            download_list_names.append(fil)\n",
    "            download_list_links.append(file_links[file_names.index(fil)])\n",
    "    #n_existing_files = len(file_names)-len(download_list)\n",
    "    return(download_list_names, download_list_links) #,n_existing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_list_names, dl_list_links = obtain_download_list(measures_antarctic_IVM_file_names, measures_antarctic_IVM_file_links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nc_direct(file_names, file_links):\n",
    "    not_downloaded_names = []\n",
    "    not_downloaded_links = []\n",
    "    print('Downloading ' + str(len(file_names)) + ' file(s)...')\n",
    "    for i in range(len(file_names)):\n",
    "        # progress report\n",
    "        print('Downloading ' + str(i + 1) + '/' + str(len(file_names)) + ': ' + file_names[i], end='\\r')\n",
    "        # get request (download file)\n",
    "        r = requests.get(file_links[i], allow_redirects=True)\n",
    "        if r.status_code != 200:    # 200 is the standard response for successful HTTP requests\n",
    "            print('ERROR: ' + str(r.status_code) + '\\n')\n",
    "            not_downloaded_names.append(file_names[i])\n",
    "            not_downloaded_links.append(file_links[i])\n",
    "        # write content to file\n",
    "        open(os.path.join(IC.data_folder,'Velocity','MEaSUREs','Annual','Data',file_names[i]), 'wb').write(r.content)\n",
    "    return(not_downloaded_names, not_downloaded_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_download_nc_direct(dl_list_names, dl_list_links):\n",
    "    # Download the files\n",
    "    not_downloaded_names, not_downloaded_links = download_nc_direct(dl_list_names, dl_list_links)\n",
    "\n",
    "    # Attempt to download any files that were not downloaded the first time\n",
    "    if len(not_downloaded_names) > 0:\n",
    "        print('The following files were not downloaded:')\n",
    "        for i in range(len(not_downloaded_names)):\n",
    "            print(not_downloaded_names[i] + ': ' + not_downloaded_links[i])\n",
    "            \n",
    "        print('Attempting to download these files once more...')\n",
    "        not_downloaded_names, not_downloaded_links = download_nc_direct(not_downloaded_names, not_downloaded_links)\n",
    "        if len(not_downloaded_names) > 0:\n",
    "            print('Please download these files manually and place them in the following folder:')\n",
    "            print(os.path.join(IC.data_folder,'Velocity','MEaSUREs','Annual','Data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 files...\n",
      "Downloading 1/1: Antarctica_ice_velocity_2005_2006_1km_v01.nc\r"
     ]
    }
   ],
   "source": [
    "run_download_nc_direct(dl_list_names, dl_list_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "changes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
