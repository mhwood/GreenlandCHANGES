{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change V002\n",
    "\n",
    "NSICD ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height V002:\n",
    "https://cmr.earthdata.nasa.gov/search/concepts/C2500138845-NSIDC_ECS.html<br>\n",
    "\n",
    "\n",
    "NSICD ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change V002: \n",
    "\n",
    "https://cmr.earthdata.nasa.gov/search/concepts/C2500140833-NSIDC_ECS.html  <br><br>\n",
    "\n",
    "\n",
    "\n",
    "NASA EarthData Search:\n",
    "\n",
    "https://search.earthdata.nasa.gov/search?q=ATL14+V002 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import requests\n",
    "import os\n",
    "import IcesheetCHANGES as changes\n",
    "import collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set options\n",
    "\n",
    "First, set collections key for desired data set from NASA EarthData and set the region name.\n",
    "For the collection ID, set collections_key using the dictionary below, or use the following instructions to add a new collection to the dictionary. \n",
    "\n",
    "Example: Getting a collection ID from EarthData Search: \n",
    "[MEaSUREs Annual Antarctic Ice Velocity Maps V001](https://search.earthdata.nasa.gov/search/granules/collection-details?p=C2245171699-NSIDC_ECS&pg[0][v]=f&pg[0][gsk]=-start_date&q=NSIDC-0720%20V001&tl=1686700071.247!3!!)\n",
    "\n",
    "While viewing the collection on EarthData, as in the above link, follow \"View More Info\" to visit the CMR page for the collection.\n",
    "Then, look for the collections ID in the URL or as a tag below the title. \n",
    "\n",
    "![Locating Collection ID from CMR page](getting_collectionID.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections: \n",
      "\n",
      "MEaSUREs Annual Antarctic Ice Velocity Maps V001\n",
      "Short name: MEaSUREs Antarctic Annual Velocity\n",
      "\n",
      "MEaSUREs Greenland Quarterly Ice Sheet Velocity Mosaics from SAR and Landsat V005\n",
      "Short name: MEaSUREs Greenland Quarterly Velocity\n",
      "\n",
      "MEaSUREs Greenland Monthly Ice Sheet Velocity Mosaics from SAR and Landsat, Version 5\n",
      "Short name: MEaSUREs Greenland Monthly Velocity\n",
      "\n",
      "ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height, Version 2\n",
      "Short name: ATL14 Antarctic Elevation\n",
      "\n",
      "ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change, Version 2\n",
      "Short name: ATL15 Antarctic Elevation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print availible collections\n",
    "collection.print_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection ID:  C2500140833-NSIDC_ECS\n"
     ]
    }
   ],
   "source": [
    "collection_key = 'ATL15 Antarctic Elevation'\n",
    "region_name = 'Antarctic'\n",
    "data_type = 'elevation'\n",
    "\n",
    "collection_id = collection.collection(collection_key)\n",
    "print('Collection ID: ', collection_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define two directories on your local drive as follows:\n",
    "\n",
    "| directory | purpose | \n",
    "|-----------|---------|\n",
    "|`project_folder` | This is the path where output data from the changes module will be stored - the data to be used directly for analysis. | \n",
    "|`data_folder` | This is the path where ice velocity and elevation data, from their respective sources, will be stored. The data_folder option was created to facilitate data storage on external drives. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/Users/tara/Documents/SJSU/MLML/Projects/CHANGES/Examples'\n",
    "data_folder='/Volumes/Seagate/CHANGES/data_repository/tutorial'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the AntarcticCHANGES object - this object will contain all pertinent information to initialize the data grids in your region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = changes.AntarcticCHANGES(project_folder, data_folder, collection_key, collection_id, region_name, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region name:\t\t Antarctic\n",
      "Data Collection:\t ATLAS/ICESat-2 L3B Gridded Antarctic and Arctic Land Ice Height Change, Version 2\n",
      "Collection short name:\t ATL15 Antarctic Elevation\n",
      "Collection ID: \t\t C2500140833-NSIDC_ECS\n",
      "Data type:\t\t Elevation\n",
      "Projection:\t\t 3031\n",
      "Download path:\t\t /Volumes/Seagate/CHANGES/data_repository/tutorial/Antarctic/Elevation/ATL15 Antarctic Elevation/Data\n",
      "Metadata path:\t\t /Users/tara/Documents/SJSU/MLML/Projects/CHANGES/Examples/Antarctic/Elevation/Metadata\n"
     ]
    }
   ],
   "source": [
    "AC.print_attributes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain list of available files for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is from the ICESat2 data page\n",
    "def cmr_filter_urls(search_results):\n",
    "    \"\"\"Select only the desired data files from CMR response.\"\"\"\n",
    "    if 'feed' not in search_results or 'entry' not in search_results['feed']:\n",
    "        return []\n",
    "    \n",
    "    entries = [e['links']\n",
    "                for e in search_results['feed']['entry']\n",
    "                if 'links' in e]\n",
    "    \n",
    "    # Flatten \"entries\" to a simple list of links\n",
    "    links = list(itertools.chain(*entries))\n",
    "\n",
    "    urls = []\n",
    "    unique_filenames = set()\n",
    "\n",
    "    for link in links:\n",
    "        if 'href' not in link:\n",
    "            # Exclude links with nothing to download\n",
    "            continue    # continue jumps to next iteration in the loop\n",
    "        if 'inherited' in link and link['inherited'] is True:\n",
    "            # Why are we excluding these links?\n",
    "            continue\n",
    "        if 'rel' in link and 'data#' not in link['rel']:\n",
    "            # Exclude links which are not classified by CMR as \"data\" or \"metadata\"\n",
    "            continue\n",
    "        if 'title' in link and 'opendap' in link['title'].lower():\n",
    "            # Exclude OPeNDAP links--they are responsible for many duplicates\n",
    "            # This is a hack; when the metadata is updated to properly identify\n",
    "            # non-datapool links, we should be able to do this in a non-hack way\n",
    "            continue\n",
    "\n",
    "        filename = link['href'].split('/')[-1]\n",
    "        \n",
    "        # Exclude links with duplicate filenames (they would overwrite)\n",
    "        if filename in unique_filenames:\n",
    "            continue\n",
    "\n",
    "        unique_filenames.add(filename)\n",
    "        urls.append(link['href'])\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMR request URL:  https://cmr.earthdata.nasa.gov/search/granules.json?echo_collection_id=C2500140833-NSIDC_ECS&page_size=2000\n",
      "Successfully obtained 56 URLs.\n"
     ]
    }
   ],
   "source": [
    "# Build and call the CMR API URL\n",
    "cmr_query_url = 'https://cmr.earthdata.nasa.gov/search/granules.json?echo_collection_id=' + AC.collection_id + '&page_size=2000'\n",
    "response = requests.get(cmr_query_url)\n",
    "\n",
    "print('CMR request URL: ', cmr_query_url)\n",
    "\n",
    "# print error code based on response\n",
    "if response.status_code != 200:\n",
    "    print('ERROR: {}'.format(response.status_code))\n",
    "search_page = response.json()\n",
    "\n",
    "# If JSON contains an error message, print the message at the key, 'error'\n",
    "if 'errors' in search_page:\n",
    "    print(search_page['errors'])\n",
    "else: \n",
    "    urls = cmr_filter_urls(search_page)\n",
    "    print(\"Successfully obtained {} URLs.\".format(len(urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only .nc files and also store the file names in a seperate list\n",
    "file_links = []\n",
    "file_names = []\n",
    "metadata = []\n",
    "\n",
    "for url in urls:\n",
    "    if '.nc'in url and not 'xml' in url:\n",
    "        file_links.append(url)\n",
    "        url_parts = url.split('/')\n",
    "        file_name = url_parts[-1]\n",
    "        file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file written to /Users/tara/Documents/SJSU/MLML/Projects/CHANGES/Examples/Antarctic/Elevation/Metadata/ATL15 Antarctic Elevation.csv\n"
     ]
    }
   ],
   "source": [
    "## write available file names and links to csv file\n",
    "metadata_folder = AC.metadata_path\n",
    "csv_filename = AC.short_name + '.csv'\n",
    "\n",
    "if not os.path.exists(metadata_folder):\n",
    "    os.makedirs(metadata_folder)\n",
    "\n",
    "f = open(os.path.join(metadata_folder, csv_filename),'w')\n",
    "f.write('File_Name,URL')\n",
    "for ea in range(len(file_names)):\n",
    "    f.write('\\n'+file_names[ea]+','+file_links[ea])\n",
    "f.close()\n",
    "\n",
    "print('CSV file written to ' + os.path.join(metadata_folder, csv_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existing files, get a list of files not on disk\n",
    "def obtain_download_list(file_names, file_links):\n",
    "    \n",
    "    output_folder = AC.download_path\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    download_list_names=[]\n",
    "    download_list_links=[]\n",
    "    for file in file_names:\n",
    "        download_file = True\n",
    "        if file not in os.listdir(os.path.join(output_folder)):\n",
    "            for existing_fil in os.listdir(os.path.join(output_folder)):\n",
    "                if file == existing_fil:  # this allows for older versions to be kept \n",
    "                    download_file = False\n",
    "        else:\n",
    "            download_file=False\n",
    "        if download_file:\n",
    "            download_list_names.append(file)\n",
    "            download_list_links.append(file_links[file_names.index(file)])\n",
    "\n",
    "    print('Number of files to download: ', len(download_list_names))\n",
    "\n",
    "    return(download_list_names, download_list_links) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files to download:  1\n"
     ]
    }
   ],
   "source": [
    "dl_list_names, dl_list_links = obtain_download_list(file_names, file_links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nc_direct(file_names, file_links):\n",
    "    if not os.path.exists(AC.download_path):\n",
    "        os.makedirs(AC.download_path)\n",
    "    \n",
    "    not_downloaded_names = []\n",
    "    not_downloaded_links = []\n",
    "\n",
    "    print('Downloading ' + str(len(file_names)) + ' file(s)...')\n",
    "    for i in range(len(file_names)):\n",
    "\n",
    "        print('Downloading ' + str(i + 1) + '/' + str(len(file_names)) + ': ' + file_names[i], end='\\r')\n",
    "\n",
    "        # get file\n",
    "        r = requests.get(file_links[i], allow_redirects=True)\n",
    "        if r.status_code != 200:    # 200 is the standard response for successful HTTP requests\n",
    "            print('ERROR: ' + str(r.status_code) + ': ' + file_links[i] + '\\n')\n",
    "            not_downloaded_names.append(file_names[i])\n",
    "            not_downloaded_links.append(file_links[i])\n",
    "        # write content to file\n",
    "        open(os.path.join(AC.download_path,file_names[i]), 'wb').write(r.content)\n",
    "    return(not_downloaded_names, not_downloaded_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_download_nc_direct(dl_list_names, dl_list_links):\n",
    "    # Download the files\n",
    "    not_downloaded_names, not_downloaded_links = download_nc_direct(dl_list_names, dl_list_links)\n",
    "\n",
    "    # Attempt to download any files that were not downloaded the first time\n",
    "    if len(not_downloaded_names) > 0:\n",
    "        print('The following files were not downloaded:')\n",
    "        for i in range(len(not_downloaded_names)):\n",
    "            print(not_downloaded_names[i] + ': ' + not_downloaded_links[i])\n",
    "            \n",
    "        print('Attempting to download these files once more...')\n",
    "        not_downloaded_names, not_downloaded_links = download_nc_direct(not_downloaded_names, not_downloaded_links)\n",
    "        if len(not_downloaded_names) > 0:\n",
    "            print('Please download these files manually and place them in the following folder:')\n",
    "            print(AC.download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1 file(s)...\n",
      "Downloading 1/1: ATL15_AA_0314_20km_002_02.nc\r"
     ]
    }
   ],
   "source": [
    "run_download_nc_direct(dl_list_names, dl_list_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "changes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
